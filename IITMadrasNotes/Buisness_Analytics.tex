\documentclass[a4paper]{article}
\input{head}

\begin{document}

\fancyhead[c]{}
\hrule \medskip
\begin{minipage}{0.295\textwidth}
\raggedright
Rishabh Indoria
\end{minipage}
\begin{minipage}{0.4\textwidth}
\centering
\LARGE
Business Analytic
\end{minipage}\
\begin{minipage}{0.295\textwidth}
\raggedleft
\today \hfill \\
\end{minipage}
\medskip \hrule
\bigskip

\section{Week 1}
	\begin{enumerate}
		\item How do you see data ?
		\begin{itemize}
			\item Good Decisions are based on an accurate understanding of Good data.
			\item Present Data in a Precise, Concise and Understandable Way.
			\item Two Types of data
			\begin{itemize}
				\item Categorical
				\item Numerical: Discrete and Continuous
			\end{itemize}
		\item Core Principle on which visualization of data is done: Nature of data dictates which visualization to use.
		\end{itemize}
		\item Benefits of visual representation of data.
		\begin{itemize}
			\item Communicate complex information concisely and precisely.
			\item Create a "picture" for reasoning about and analysing quantitative and conceptual information.
			\item Provides "Information Rich View" at a glance.
			\item Directs attention towards content rather than methodology.
			\item Describe, Explore and Summarize a set of numbers.
			\item Convey messages about significance of data.
		\end{itemize}
		\item 4 Principles of effective visualization.
		\begin{itemize}
			\item Know Purpose
			\item Ensure Integrity
			\item Maximize data ink and Minimize non data ink
			\item Show your data, annotate
		\end{itemize}
		\item Executing your information display is a 3 step process.
		\begin{itemize}
			\item Defining the message
			\begin{itemize}
				\item What am I trying to communicate ?
				\item Should I use text, table, graph or a combination ?
				\item The message/statistic you want to emphasize
			\end{itemize}
			\item Choosing Form
			\begin{itemize}
				\item What is the message ?
				\item What design principles lead to quick cognitive processing \& effective communication ?
				\item Whether to display the data as a table or a chart
			\end{itemize}
			\item Creating Designs
			\begin{itemize}
				\item How do I make the message clear at a glance ?
				\item Avoid 3D effects, Avoid legends(Use Labels), Avoid contrasting borders around objects, Use annotations to highlight key data changes or to focus on specific data points.
			\end{itemize}
		\end{itemize}
		\item Dashboard
		\begin{itemize}
		\item A visual display of the most important information needed to achieve one or more objective that has been consolidated on a single screen so it can be monitored \& understood at a glance.
			\item Scan the big picture, Zoom in on important specifics, Link to supporting details.
		\end{itemize}
	\end{enumerate}
\section{Week 2}
	\begin{enumerate}
		\item Probability Distributions
		\begin{itemize}
			\item Trace-Driven Simulation: Data values themselves used directly in simulations.
			\item Fit: Use a theoretical distribution for the data.
			\item Data values could be used to define empirical distribution.  
		\end{itemize}
		\item Empirical Distributions
		\begin{itemize}
			\item Using data we build our own distributions.
			\item Define density/Distribution function
			\item Estimate Parameters
			\item Ungrouped data: $X_1 \leq X_2 \leq X_3 \leq ... \leq X_n$
			\[
			E(x) = 
			\begin{cases}
				0&\text{for $x < X_1$}\\
				\frac{i - 1}{n - 1} + \frac{x - X_i}{(n - 1)(X_{i+1} - X_i)}&\text{for $X_i \leq x < X_{i+1}$, $i = 1,2,...,n-1$}\\
				1&\text{for $X_n \leq x$}
			\end{cases}				
			\]
			\item Grouped Data : $n X_j^{'}$s are grouped in $k$ adjacent intervals so that the $jth$ interval contains $nj$ observations, $n_1 + n_2 + ... + n_k = n$
			
			Intervals: $(a_0, a_1), (a_1, a_2), ..., (a_{k-1}, a_k)$, 
			
			$G(a_0) = 0, G(a_j) = \frac{n_1 + n_2 + n_3 + ... + n_j}{n}$
			\[
			G(x) = 
			\begin{cases}
			0&\text{for $x < a_0$}\\
			G(a_{j-1}) + \frac{x-a_{j-1}}{a_j-a_{j-1}}[G(a_j) - G(a_{j-1}]&\text{for $a_{j-1} \leq x < a_j$, $j = 1,2,3,...,k$}\\
			1&\text{for $a_k \leq x$}
			\end{cases}
			\] 
		\end{itemize}
		\item Clues from summary statistics
		\begin{itemize}
			\item Symmetric distributions: mean $\approx$ median, eg: Normal Distribution
			\item Coefficient of Variation(cv): Ratio of Standard deviation \& mean, $\frac{\sigma}{\mu}$
			
			Continuous Distributions: cv $\approx$ 1, eg: Exponential Distribution
			
			Right/Positive skewed histogram: cv $>$ 1, eg: log normal distribution
			\item Lexi$^{'}$s ratio: Same as cv for Discrete Distributions.
			\item Skewness(v): Measure of symmetry of a distribution
			
			v $=$ 0, Normal Distribution
			
			v $>$ 0, right skewed(exponential distribution)
			
			v $<$ 0, left skewed 
		\end{itemize}
		\item Parameter Estimation
		\begin{itemize}
			\item Once distribution is guessed, next step is estimating parameters of the distribution.
			\item Most common method used is MLE.
		\end{itemize}
		\item Goodness of Fit
		\begin{itemize}
			\item Can be checked by
			\begin{itemize}
				\item Frequency Comparison(a bit technical)
				\item Probability Comparison(Visual tool)
				\item Goodness of Fit test(statistical test for goodness) 
			\end{itemize}
			\item Quantile-Quantile Plot(Q-Q Plot)
			\begin{itemize}
				\item Graph of q$_{i}$ quantile of model vs q$_{i}$ quantile of sample distribution.
				\item $x_{q_{i}}^{M} = \hat{F}^{-1}(q_{i})$
				\item $x_{q_{i}}^{S} = \tilde{F}^{-1}(q_{i}) = x_{i}, i = 1,2,3,...$
				\item If our distribution is correct, then we will get a line with slope 1 and intercept 0 (Linear) \& $x_{q_{i}}^{M} \approx x_{q_{i}}^{S}$
				\item Amplifies difference between the tails of model distribution.\\
			\end{itemize}
			\item Probability-Probability Plot(P-P Plot)
			\begin{itemize}
				\item Graph of model Probability $\hat{F}(X_{i})$ vs Sample Probability $\tilde{F}_{n}(X_{i})$
				\item Valid for both Continuous and Discrete data sets.
				\item I chosen distribution is correct then $\hat{F}(X_{i})\approx \tilde{F}_{n}(X_{i})$, the plot will be linear with slope 1 and intercept 0.
				\item Amplifies differences between middle portion of the model distribution.
			\end{itemize}
			\item Goodness of fit tests
			\begin{itemize}
				\item Statistical Hypothesis test that is used to assess formally whether observations are independent samples from a particular distribution.
				\item $H_{0}$: Observation are independent.
				\item Chi-Square Test
				\begin{itemize}
					\item Require frequency tables: Bins, Object Frequency, Expected frequency.
					\item Calculate test statistic $\chi^{2} = \frac{\Sigma(O_{i} - E_{i})^{2}}{E_{i}}$
					\item Compute p-value, if it is less than significant level($\alpha$) then reject $H_{0}$
					\item Compute tabulated $\chi^{2}_{k - p - 1, \alpha}$, if $\chi^{2}_{tabulated} < \chi^{2}_{computed}$ then reject $H_{0}$.
					
					p = number of parameters
					
					k = number of bins
				\end{itemize}
			\end{itemize}
		\end{itemize}
	\end{enumerate}
\section{Week 3}
	\begin{enumerate}
		\item Ordinal Data: Categorical data which can be ordered.
		\item Conditional Probability: $\frac{\text{Joint Probability}}{\text{Marginal Probability}}$
		\item Conditional Probability can be compared using Joint Probability table(Contingency Table)
		\item Baye$^{'}$s Rule
		\begin{itemize}
			\item Posterior Probability can be found using initial probability and additional information
			\item $P(A\cap B) = P(A|B)P(B)$
			\item $P(A|B) = \frac{P(A)P(B|A)}{P(B)}$
		\end{itemize}
		\item Chi-Squared Test of Independence
		\item Null Hypothesis $H_0$: Categorical Variables are independent.
		\item Alternate Hypothesis $H_1$: Categorical variables are not independent.
		\item Example Table:
		
		\begin{tabular}{|c||c|c|c||c|}
			\hline
			City$\backslash$Preferred Brand& Brand A& Brand B& Brand C& Total\\
			\hline \hline
			Mumbai& 279& 73& 225& 577\\
			\hline
			Chennai& 165& 47& 191& 403\\
			\hline \hline
			Total& 444& 120& 416& 980\\
			\hline
		\end{tabular}
		\begin{itemize}
			\item Independent(Explanatory) Variable is City
			\item Dependent(Response) Variable is Brand Preference
			\item $f_{o}$: Observed Frequency(from Samples)
			\item $f_{e}$: Expected Frequencies, if variables were independent = $\frac{\text{Row Total $\cdot$ Column Total}}{\text{Overall Total}}$
			\item degree of freedom = (number of rows - 1) $\cdot$ (number of columns - 1)
		\end{itemize}
	\end{enumerate}
\section{Week 4}
	\begin{enumerate}
		\item Demand Response Curve
		\begin{itemize}
			\item Properties: Non-Negative, Continuous \& Differentiable and Generally Downwards Slopping
			\item Price Sensitivity = $\frac{D(P_{2}) - D(P_{1})}{P_{2} - P_{1}}$
			\item Demand Elasticity = $-\frac{\frac{D(P_{2}) - D(P_{1})}{D(P_{1})}}{\frac{P_{2} - P_{1}}{P_{1}}}$\\
		\end{itemize}
		\item Linear Response Curve
		\begin{itemize}
			\item $D(P) = D_{o} - mP$
			\item Satiating Price $P_{s} = \frac{D_{o}}{m}$; $D(P_{s}) = 0$
			\item Demand at $P = 0$ is $D_{o}$
			\item Elasticity $\varepsilon = \frac{mP}{D_{o} - mP}$ 
		\end{itemize}
		\item Constant Elasticity Curve
		\begin{itemize}
			\item $D(P) = cP^{-\varepsilon}$
			\item $c$ = Demand when $P = 1$
			\item Revenue R = $P\cdot D(P)$
		\end{itemize}
		\item Elasticity
		\begin{itemize}
			\item $\varepsilon < 1$: Inelastic Product Demand, Increase Revenue $\implies$ Increase Price
			\item $\varepsilon > 1$: Elastic Product Demand, Increase Revenue $\implies$ Decrease Price
		\end{itemize}
		\item Simple Linear Regression can be used to fir Linear curves
		\begin{itemize}
			\item Loss/Error = $\frac{\Sigma (y - \hat{y})^{2}}{N}$
			\item error term $e = (y - \hat{y})^{2} \sim N(0,\sigma_{e}^{2})$
			\item error terms are independent, have equal variance and are normally distributed. 
		\end{itemize}
	\end{enumerate}
 \section{Week 5}
 \begin{enumerate}
     \item Constant Elasticity Model
     \begin{itemize}
         \item $D(P) = cP^{-\epsilon}\implies \log(D(P)) = \log(c) - \epsilon\log(P)$
         \item Transformation based on which quadrant graph lies in \\
         Ist Quadrant: $(x,y) \longrightarrow (x^2,y^2)$\\
         IInd Quadrant: $(x,y) \longrightarrow (\log(x),y)$ or $(\frac{1}{x},y)$\\
         IIIrd Quadrant: $(x,y) \longrightarrow (\log(x),\log(y))$ or $(\frac{1}{x},\frac{1}{y})$\\
         IVth Quadrant: $(x,y) \longrightarrow (x,\log(y))$ or $(x,\frac{1}{y})$
     \end{itemize}
     \item Performance Metrics
     \begin{itemize}
         \item $R^2$: Amount of variability explained\\
         Formula: $1 - \frac{sum squared regression(SSR)}{total sum of squares(SST)}$ where, SSR: $\Sigma(y_i - \hat{y}_i)^2$ and SST: $\Sigma(y_i - \Bar{y}_i)^2$
         \item Multiple R: Coefficient of Correlation, Strength of Association\\
         Formula: $\sqrt{R^2}$
         \item Std Error: Standard Deviation of Error Terms
         \item F Statistic: $\frac{MSE}{MS of Regression}$
         \item Confidence Level: Generally $0.95$
         \item $\alpha$ value: 1 - Confidence Level. If it is greater than p-value then we reject null hypothesis $H_0$
     \end{itemize}
     \item Optimal Pricing
     \begin{itemize}
         \item Revenue Maximization\\
         Revenue = $P.D(P)$\\
         Set $\frac{\delta R(P)}{\delta P} = 0$ and $\frac{\delta^2R(P)}{\delta P^2} < 0$
         \item Profit Maximization\\
         Profit = Revenue - Cost\\
         Assuming Marginal Cost = C $\implies \pi(P) = P.D(P) - C.D(P)$
     \end{itemize}
 \end{enumerate}
 \section{Week 6}
 \begin{enumerate}
     \item Multiple Linear Regression
     \begin{itemize}
         \item $y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \epsilon$, $\epsilon ~ N(0,\sigma_\epsilon^2)$
         \item Adjusted $R^2$: Adjusts for the amount of $X_i$\\
         Formula: $1 - \frac{(1 - R^2)(N - 1)}{N - p - 1}$, where $N$ is the total sample size and $p$ is the number of independent variables.
         \item Calibration Plot: Scatter Plot between $y$ and $\hat{y}$
         \item R: Correlation between $y$ and $\hat{y}$
         \item Col-linearity: Very high correlation between explanatory variables.
         \item Path Diagram: Schematic Drawing of relationship among explanatory variables and response
     \end{itemize}
     \item Variance Inflation Factor
     \begin{itemize}
         \item Quantifies amount of unique variation in each explanatory variable and measures effect of col-linearity
         \item VIF of $X_j$ = $\frac{1}{1-R_j^2}$
         \item $R_j^2$ is coefficient of determination in regression of $X_j$ on ALL other explanatory variables.
         \item $se(b_1) = \frac{se}{\sqrt{n}}.\frac{1}{s_x}$, with VIF $se(b_1) = \frac{se}{\sqrt{n}}.\frac{\sqrt{VIF(X_1)}}{s_x}$
     \end{itemize}
 \end{enumerate}
 \section{Week 7}
 \begin{enumerate}
     \item Logistic Regression(Classification)
     \begin{itemize}
         \item ODDs(Success in some event) = $\frac{Pr(Y=1)}{Pr(Y=0)}$
         \item logit = $\log(ODDs) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$
         \item $Pr(Y=1)$ = $\hat{y}$ = $\frac{e^{w_0 + w_1x_1 + w_2x_2 + ...}}{1 + e^{w_0 + w_1x_1 + w_2x_2 + ...}}$
         \item We are going to maximize log likelihood or minimize negative log likelihood\\
         minimize $-(y\log(\hat{y}) + (1-y)\log(1-\hat{y})$
         \item if $\hat{y}\geq $ cutoff(generally around $0.5$) then prediction is $1$ else $0$
         \item Performance Metrics\\
         \begin{tabular}{|c||c|c|}
			\hline
			& Predicted $1$& Predicted $0$\\
			\hline \hline
			Actual $1$& True Positive(TP)& False Negative(FN)\\
			\hline
			Actual $0$& False Positive(FP)& True Negative(TN)\\
			\hline
		\end{tabular}\\
        Accuracy: $\frac{TP + TN}{Total}$\\
        Precision: $\frac{TP}{TP+FP}$\\
        Sensitivity/Recall: $\frac{TP}{TP+FN}$\\
        F1 Score: $\frac{2.Precision.Recall}{Precision+Recall}$
     \end{itemize}
 \end{enumerate}
 \section{Week 8}
 \begin{enumerate}
     \item Efficiency(Productive Efficiency)
     \begin{itemize}
         \item Effective utilization of resources for maximization of benefits
         \item Productive efficiency is an aspect of economic efficiency focusing on maximizing output under given conditions.
         \item Productive efficiency "frontier" are all combination of o/p such that the production of one product cannot be increased without sacrificing the output of the other.
     \end{itemize}
     \item Efficiency Measurement
     \begin{itemize}
         \item In simplest way: $\frac{output}{input}$
         \item But there can be multiple types of inputs: Labor, Infrastructure, Money(Resources go as inputs)
         \item Outputs can be customers served/acquired, Profits, Sales volume
         \item How has the organization performed is the output\\
     \end{itemize}
     \item Optimization Method - Data Envelopment Analysis
     \begin{itemize}
         \item Non Parametric Mathematical method to find production frontier. Used to calculate productive efficiency of an economic unit. Economic unit is refereed to as "Decision Making Unit". Measures Relative Efficiency, formulates optimization problem for each DMU
         \item DEA Logic
         \begin{itemize}
             \item For multiple inputs and outputs define weighted ratio. Some inputs cannot be directly added, define weight for each input. Similar for outputs.
             \item Let each DMU choose input and output weights to its advantage
             \item For each DMU:-
             Objective constraint: Maximize its efficiency by choosing its weights carefully\\
             Efficiency $\epsilon[0,1]$ and using these weights none of the other DMU's should get efficiency $> 1$\\
             Using these weights is it cannot achieve an efficiency of $1$, then this DMU is truly inefficient.\\
             Using these weights if a different DMU gets an efficiency of 1, then that DMU is really good.
         \end{itemize}
         \item DEA Mathematical Formula\\
         K = Number of DMU's considered in the data set\\
         N = Number of inputs considered, M = Number of outputs considered\\
         $I_{ik}$ = Recorded value of input i for DMU k, $O_{jk}$ = Recorded value of output j for DMU k\\
         $x_{ik}$ = Weight assigned to input i by DMU k, $y_{jk}$ = Weight assigned to output j by DMU k \\
         $E_k$ = Efficiency of DMU k = $\frac{Weighted Output}{Weighted Input}$ = $\frac{\Sigma y_{jk}O_{jk}}{x_{ik}I_{ik}}$\\
         Subject to $E_k < 1, k  = 1, 2, ..., K$. Since, Linear problems are much easier to solve we linearize it.\\
         For each DMU: Max $\Sigma y_{jk}O_{jk}$ for all j\\
         Subject to $\Sigma x_{ik}I_{ik} = 1$\\
         $\Sigma y_{j1}O_{j1} \leq \Sigma x_{i1}I_{i1}$\\
          $\Sigma y_{j2}O_{j2} \leq \Sigma x_{i2}I_{i2}$\\
          $...$\\
           $\Sigma y_{jK}O_{jK} \leq \Sigma x_{iK}I_{iK}$
     \end{itemize}
 \end{enumerate}
 \section{Week 9}
 \begin{enumerate}
     \item Prescription for inefficient DMU
     \begin{itemize}
        \item Important Economic Concepts: Types of Efficiencies, and Disposable inputs/outputs
         \item The DMU which is NOT on the efficiency frontier should move in both horizontal AND vertical direction.
         \item Draw a line from origin to inefficient DMU, then move the DMU along the line towards the efficiency frontier. The point of intersection of this line and efficiency frontier is called Hypothetical Composite Unit(HCU).
         \item The efficient DMU's which make up the part of the frontier where HCU resides are the reference DMU's for the inefficient one. In terms of optimization problem, we get the value of dual variable or shadow price, the variables whose value is not 0 correspond to the reference DMU.
     \end{itemize}
 \end{enumerate}
 \section{Week 10 and 11}
 \begin{enumerate}
     \item Consumer Choice Model
     \begin{itemize}
         \item Thinking Process: Consider a consumer comparing four products, we have data for two attributes on these variants, we also have consumer choices data available for us(Consumer provides his/her choices).
         \item Marketer would like to know how important each attribute is.
     \end{itemize}
     \item Conjoint Analysis
     \begin{itemize}
         \item It is the analysis of features considered jointly. It constructs a value system and can be used to arrive at the "best" product.\\
         A Family of techniques that model choice by decomposing overall preference in terms of relative values of components or attributes to respondents.
         \item Forms
         \begin{itemize}
             \item Choice Based Conjoint(CBC) Analysis: Most Common, Customer chooses most preferred full profile product amongst 3-4 options.
             \item Adaptive Conjoint Analysis(ACA): Each customer is asked different set of questions which are dynamically decided based on their responses.
             \item Full Profile: Full suite of options are presented to the customer and their preference is sought.
             \item Menu-based: Customer is shown a list of attributes with associate prices, Customer then chooses what they want in their ideal product.
         \end{itemize}
         \item Applications
         \begin{itemize}
             \item Marketing: Once Attributes most preferred by customers are known, these can be highlighted in communication channel. Consumers may differ in their choices of preferred attributes and hence CA can help in segmenting the market.
             \item Product Development: The product dev team can focus on refining these attributes.
             \item Pricing: Organization can decide to price the product based on level of attribute present. May also reveal consumers willingness to pay(WTP) for particular attributes.
         \end{itemize}
         \item The Process: By defining products as a collection of attributes and having individual consumer react to a number of alternatives. One can infer each attributes importance and most desired level for each customer.
         \item Conjoint Problem\\
         Product options are represented as points in multi-attribute space. Different consumer correspond to different "ideal points" that denote their "most preferred". Consumer is supposed to prefer the option which is "closer" to this ideal. As a measure of distance normally either the Euclidean metric or the weighted Euclidean metric is used.\\
         J = \{$1, 2, 3, ..., n$\} = set of options\\
         P = \{$1,2,...,t$\} = n options are described in terms of t dimensions(number of attributes)\\
         $Y_j$ = \{$y_{jp}$\} = pre specified location of jth option in the t dimensional space\\
         $X$ = \{$x_p$\} = ideal point of consumer\\
         $d_j^u$ = $\sqrt{\Sigma(y_{jp} - x_p)^2}$ = Unweighted distance, $d_j^w$ = $\sqrt{\Sigma w_p(y_{jp}x_p)^2}$ = Weighted distance\\
         Weights are non negative for all attributes, $s_j = (d_j^w)^2$ = Squared distance\\
         $\Omega$ = \{$j,k$\} denote the set of ordered pairs\\
         Constraint: $s_k \geq s_j$, B = Poorness of fit = $\Sigma(s_j - s_k)^+ $ = max$(0,s_j-s_k)$
         \item Final Formulation\\
         $a_{jkp}$ = $y_{kp}^2 - y_{jp}^2$ $\forall (j,k)\epsilon \Omega$, $A_p$ = $\Sigma a_{jkp}$\\
         $b_{jkp}$ = $-2(y_{kp} - y_{jp})$, $D_p$ = $\Sigma b_{jkp}$\\
         $V$ = \{$v_p$\} = \{$w_px_p$\}\\
         $z_{jk}$ = max$[0, -[\Sigma w_pa_{jkp} + \Sigma v_pb_{jkp}]]$\\
         Min $\Sigma z_{jk}$\\
         Subject to: $\Sigma w_pa_{jkp} + \Sigma v_pb_{jkp} + z_{jk} \geq 0$\\
         $\Sigma w_pA_p + \Sigma v_pD_p$ = $1$
         \item Statistical Method - Linear Regression
         \begin{itemize}
             \item Traditionally CA is just a multiple regression problem. Customers ratings for the product concepts form the dependant variables, characteristics of the product are independent variable.
             \item Estimated Betas associated with the independent variable are the utilities.
             \item R-Square for the regression characterizes the internal consistency of the respondent.
             \item Use one hot encoding then remove one column to eliminate col linearity.
             \item Part worth: Level utilities for attributes. Total worth of product is calculated from multiple attributes and multiple levels of attributes together, utility values for separate parts pf the product are part worth.
             \item Importance of each attribute: Calculate the range of part worth, then find total of the ranges, then the importance = $\frac{range}{total}$
         \end{itemize}
     \end{itemize}
 \end{enumerate}
\end{document}
