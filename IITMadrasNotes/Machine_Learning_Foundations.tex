\documentclass[a4paper]{article}\input{head}

\begin{document}

\fancyhead[c]{}
\hrule \medskip
\begin{minipage}{0.295\textwidth}
\raggedright
Rishabh Indoria
\end{minipage}
\begin{minipage}{0.4\textwidth}
\centering
\LARGE
Machine Learning Foundations
\end{minipage}\
\begin{minipage}{0.295\textwidth}
\raggedleft
\today \hfill \\
\end{minipage}
\medskip \hrule
\bigskip

\section{Week 1}
	\begin{enumerate}
		\item Supervised Learning: Regression
		\begin{itemize}
			\item Find model $f$ such that $f(x^{i})\approx g^{i}$
			\item Training Data: ${(x^{1},y^{1}),(x^{2},y^{2}),...,(x^{n},y^{n})}$
			\item Loss = $\frac{\Sigma (f(x^{i}) - y^{i})^{2}}{n}$
			\item $f(x) = w^{T}\cdot x + b$
		\end{itemize}
		\item Supervised Learning: Classification
		\begin{itemize}
			\item $y^{i}\epsilon {-1, +1}$
			\item Loss = $\frac{\Sigma 1(f(x^{i})\neq y^{i})}{n}$
			\item $f(x) = sign(w^{T}\cdot x + b)$
		\end{itemize}
		\item Validation Data: Choosing the right collection of models is done using validation data.
		\item Unsupervised Learning: Dimensionality Reduction
		\begin{itemize}
			\item Data = ${x^{1},x^{2},...,x^{n}}$
			\item Compress, Explain and Group Data.
			\item Encoder $f:R^{d}\rightarrow R^{d'}$ Decoder: $f:R^{d'}\rightarrow R^{d}$
			\item Goal: $g(f(x^{i})) \approx x^{i}$
			\item Loss: $\frac{\begin{Vmatrix} g(f(x^{i}))-x^{i} \end{Vmatrix}^{2}}{n}$
		\end{itemize}
		\item Unsupervised Learning: Density Estimation
		\begin{itemize}
			\item Probabilistic Model
			\item $P:R^{d}\rightarrow R_{+}$ that sums to 1
			\item $P(x)$ is large is $x\epsilon$ Data and low otherwise
			\item Loss: $\frac{\Sigma -log(P(x^{i}))}{n}$
		\end{itemize}
	\end{enumerate}
\section{Week 2}
	\begin{enumerate}
		\item Continuity \& Differentiability
		\begin{itemize}
			\item $f:R\rightarrow R$ is continuous if $\lim_{x\to x^{*}}f(x) = f(x^{*})$
			\item Differentiable if $\lim_{x\to x^{*}}\frac{f(x) - f(x^{*})}{x - x^{*}} = f(x')$ exists.
			\item if $f$ is NOT continuous $\implies$ NOT differentiable
		\end{itemize}
		\item Linear Approximation
		\begin{itemize}
			\item If $f$ is differentiable
			\item $f(x)\approx f(x^{*}) + f'(x^{*})(x - x^{*})$
			\item $f(x)\approx L_{x^{*}}[f](x)$
		\end{itemize}
		\item Higher order Approximation
		\begin{itemize}
			\item $f(x)\approx f(x^{*}) + f'(x^{*})(x - x^{*}) + \frac{f''(x^{*})}{2!}(x - x^{*})^{2} + ...$
		\end{itemize}
		\item Lines
		\begin{itemize}
			\item Line through point $u$ along vector $v = {x, x = u + \alpha v}$, where $u,v,x\epsilon R^{d}$ and $\alpha \epsilon R$
			\item Line through points $u$ and $u'$ = ${x, x = u + \alpha (u - u')}$
		\end{itemize}
		\item Hyper Planes
		\begin{itemize}
			\item Hyper Plane normal to vector $w$ with value $b$ = ${x, w^{T}\cdot x = b}$, where $x, w\epsilon R^{d}$ and $b\epsilon R$
		\end{itemize}
		\item Partial Derivatives \& Gradients
		\begin{itemize}
			\item $f:R^{d}\rightarrow R$
			\item $\frac{\delta f}{\delta x}(v) = [\frac{\delta f}{\delta x_{1}}(v), \frac{\delta f}{\delta x_{2}}(v),...,\frac{\delta f}{\delta x_{d}}(v)]$
			\item $\Delta f(v) = [\frac{\delta f}{\delta x}]^{T}$
		\end{itemize}
		\item Multivariate Linear Approximation
		\begin{itemize}
			\item $f(x)\approx f(v) + \Delta f(v)^{T}(x - v) = L_{v}[f](x)$
		\end{itemize}
		\item Directional Derivative
		\begin{itemize}
			\item $D_u[f](v) = \frac{\delta f}{\delta x}(v)^{T}\cdot u$, at point $v$ along $u$
		\end{itemize}
		\item Direction of steepest ascent
		\begin{itemize}
			\item Find $u\epsilon R^{d}$, $\begin{Vmatrix} u \end{Vmatrix} = 1$ \& maximize $D_{u}[f](v)$
			\item $u = \alpha \cdot \Delta f(v)$
		\end{itemize}
	\end{enumerate}
\section{Week 3}
	\begin{enumerate}
		\item Four Fundamental Sub Spaces
		\begin{itemize}
			\item Column Space $C(A)$
			\begin{itemize}
				\item span($u_{1}, u_{2},...,u_{n}$) = Linear Combination of vectors
				\item If $Ax = b$ has a solution, then $b\epsilon C(A)$
				\item Rank = number of pivot columns = dim($C(A)$)
			\end{itemize}
			\item Null Space $N(A)$
			\begin{itemize}
				\item ${x| Ax = 0}$
				\item If $A$ is invertible then $N(A)$ only contains zero, and $Ax = b$ has a unique solution.
				\item Nullity = number of free variables = dim($N(A)$)
				\item If $A$ has $n$ columns, then rank + nullity = $n$
				\item Can use Gaussian Elimination to solve for $N(A)$
			\end{itemize}
			\item Row Space $R(A)$
			\begin{itemize}
				\item Column Space of $A^{T}$
				\item Column Rank $dim(C(A))$ = Row Rank $dim(R(A))$
				\item $R(A) \perp N(A)$
			\end{itemize}
			\item Left Null Space $N(A^{T})$
			\begin{itemize}
				\item $C(A) \perp N(A^{T})$
			\end{itemize}
		\end{itemize}
		\item Orthogonal and Vector Sub Spaces
		\begin{itemize}
			\item Orthogonal Vectors, $x \perp y$ if $x\cdot y = x^{T}y = 0$
			\item Orthonormal Vectors, $u \perp v$ and $\begin{Vmatrix} u\end{Vmatrix} =\begin{Vmatrix} v\end{Vmatrix} = 1$
		\end{itemize}
		\item Projections
		\begin{itemize}
			\item Projection onto a line
			\begin{itemize}
				\item $p = \hat{x}a$
				\item $e = b - p = b - \hat{x}a$
				\item $e \perp a \implies \hat{x} = \frac{a^{T}b}{a^{T}a}$
				\item Projection matrix $P = \frac{aa^{T}}{a^{T}a}$
				\item $p = Pb$
				\item $P$ is symmetric, $P^{2} = P$, Rank $P = 1$
			\end{itemize}
			\item Projection onto a subspace
			\begin{itemize}
				\item Projection of $b$ onto $C(A)$, $Ax = b$
				\item $p = A\hat{x}, e = b - A\hat{x}$
				\item $e \perp$ every vector in $C(A)$ and $N(A^{T}) \perp C(A) \implies e\epsilon N(A^{T})$
				\item Projection Matrix $P = A(A^{T}A)^{-1}A^{T}$, $p = Pb$
			\end{itemize}
		\end{itemize}
		\item Least Squares
		\begin{itemize}
			\item Suppose we have a vector $b$ which leads to an inconsistent system $Ax\neq b$
			\item Next best thing we do is minimize average error, $E^{2} = (Ax-b)^{2}$
			\item $\frac{\delta E^{2}}{\delta x} = 0 \implies (A^{T}A)x = A^{T}b$ 
		\end{itemize}
	\end{enumerate}
\section{Week 4}
	\begin{enumerate}
		\item Linear and Polynomial Regression
		\begin{itemize}
			\item Minimize Loss $L(\theta) = \frac{\Sigma (x_{i}^{T} - y_{i})^{2}}{2}$
			\item Use least squares method $(A^{T}A)\theta = A^{T}Y$
			\item Polynomial Regression
			\begin{itemize}
				\item Transformed Features: $\hat{y}(x) = \theta_{0}+\theta_{1}x + \theta_{2}x^{2} + \theta_{m}x^{m} = \Sigma \theta_{j}\phi_{j}(x), \phi_{j}(x) = x^{j}$
				\item $\hat{y}(x) = \theta^{T}\phi(x)$, $(A^{T}A)\theta = A^{T}Y$
				\item Then Proceed as Linear Regression
			\end{itemize}
			\item Regularized Loss
			\begin{itemize}
				\item $\bar{L}(\theta) = \frac{(x^{T}_{i}\theta - y_{i})^{2}}{2} + \lambda \begin{Vmatrix} \theta \end{Vmatrix}^{2}$, Regularized Term = $\lambda \begin{Vmatrix} \theta \end{Vmatrix}^{2}$
				\item $(A^{T}A + \lambda I)\theta_{reg} = A^{T}Y$
				\item Overfitting $\rightarrow$ Too small $\lambda$
				\item Underfitting $\rightarrow$ Too large $\lambda$
			\end{itemize}
		\end{itemize}
		\item Eigenvalues and Eigenvectors
		\begin{itemize}
			\item Eigenvalue equation $Ax = \lambda x$
			\item $\frac{\delta u}{\delta t} = Au$ can be solved with solutions of the form $u(t) = e^{\lambda t}x$ if $Ax = \lambda x$
			\item $(A - \lambda I)x = 0$
			
			Characteristic polynomial $|A - \lambda I| = 0$
			
			Trace of $A = \Sigma \lambda$ = Sum of diagonal elements of $A$
			
			$|A|$ = Determinant of $A$ = $\Pi \lambda$ 
		\end{itemize}
		\item Diagonalization of a Matrix
		\begin{itemize}
			\item A matrix $A$ is diagonalizable if there exists an invertible matrix $S$ such that $S^{-1}AS = \lambda$, $\lambda = $ Diagonal Matrix
			\item $S = \begin{bmatrix}x_{1}&x_{2}&...&x_{n}\end{bmatrix}$, $x_{1}, x_{2}, ..., x_{n} =$ eigenvectors
			\item $S^{-1}A^{k}S = \lambda^{k}$, $k \geq 1$
			\item $Q\lambda Q^{T} = A$
			
			$Q = \begin{bmatrix}q_{1}&q_{2}&...&q_{n}\end{bmatrix}$
			
			$q_{1} = \frac{x_{1}}{\begin{Vmatrix}x_{1}\end{Vmatrix}}, q_{2} = \frac{x_{2}}{\begin{Vmatrix}x_{2}\end{Vmatrix}}, ..., q_{n} = \frac{x_{n}}{\begin{Vmatrix}x_{n}\end{Vmatrix}}$
		\end{itemize}
		\item Fibonacci Sequence $F_{k}\approx \frac{1}{\sqrt{5}}(\frac{1+\sqrt{5}}{2})^{k}$
	\end{enumerate}
\section{Week 5}
	\begin{enumerate}
		\item Complex Matrices
		\begin{itemize}
			\item $C^{n}$: Complex counter part of $R^{n}$
			\item inner product $x\cdot y = \bar{x}^{T}y$
			
			$\bar{x}^{T}y \neq \bar{y}^{T}x$
			
			$\begin{Vmatrix}x\end{Vmatrix}^{2} = \bar{x}^{T}x$
			\item $A^{*} = $Conjugate Transpose of $A = \bar{A}^{T}$
		\end{itemize}
		\item Hermitian Matrix
		\begin{itemize}
			\item $A^{*} = A$, equivalent of symmetric matrices in complex
			\item All Eigenvectors are real and orthogonal
		\end{itemize}
		\item Unitary Matrix
		\begin{itemize}
			\item $U^{*}U = I$
			\item $\begin{Vmatrix}Ux\end{Vmatrix} = \begin{Vmatrix}x\end{Vmatrix}$
			\item $U^{-1} = U^{*}$
			\item $|\lambda | = 1$, where $\lambda$ is any eigenvalue
		\end{itemize}
		\item Diagonalization of Hermitian Matrices
		\begin{itemize}
			\item $A$ is unitary diagonalizable if $A = U\lambda U^{*}$
			\item Any $n\times n$ matrix $A$ is similar to an $n\times n$ upper triangular matrix, $A = UTU^{*}$
			\item If $U_{1} = \begin{bmatrix}w_{1}&w_{2}&...\end{bmatrix}$ is the matrix then take $w_{1} = X_{1}$, first eigenvector then $w_{2} = X_{2} - \frac{w_{1}\cdot X_{2}}{\begin{Vmatrix}w_{1}\end{Vmatrix}^{2}}w_{1}$ 
		\end{itemize}
	\end{enumerate}
\section{Week 6}
	\begin{enumerate}
		\item Singular Value Decomposition
		\begin{itemize}
			\item Let $A$ be a real symmetric matrix
			
			Then all eigenvalues of $A$ are real and $A$ is orthogonally diagonalizable
			
			$A = Q\lambda Q^{T}$, $Q^{T}Q = I$
			\item Any real $m\times n$ matrix $A$ can be decomposed to SVD form
			
			$A(m\times n) = Q_{1}(m\times m)\Sigma (m\times n)Q_{2}(n\times n)$, $Q_{1}^{T}Q_{1} = I$, $Q_{2}^{T}Q_{2} = I$
			
			$Q_{1}$\&$Q_{2}$ are orthogonal
			\item $\Sigma = \begin{bmatrix}D&0\\
			0&0\end{bmatrix}$, where $D = \begin{bmatrix}\sigma_{1}&0&0&...&0\\
			0&\sigma_{2}&0&...&0\\
			0&0&0&...&\sigma_{r}\end{bmatrix}$
			\item $\sigma_{i}$ are called singular values and $\sigma_{i} = \sqrt{\lambda_{i}}$
			
			where $\lambda_{i}$ are eigenvalues of $A^{T}A$ and $x_{i} are eigenvectors$
			\item Let $y_{i} = \frac{A_{i}x_{i}}{\sigma_{i}}$
			\item $Q_{1} = \begin{bmatrix}y_{1}&y_{2}&...&y{m}\end{bmatrix}$, where $y_{i}$ are eigenvectors of $AA^{T} = Q_{1}\Sigma \Sigma^{T}Q_{1}^{T}$ and
			
			$Q_{2} = \begin{bmatrix}x_{1}&x_{2}&...&x{m}\end{bmatrix}$, where $x_{i}$ are eigenvectors of $A^{T}A = Q_{2}\Sigma^{T}\Sigma Q_{2}^{T}$
		\end{itemize}
		\item Positive Definite
		\begin{itemize}
			\item A function $f$ that vanishes at $(0,0)$ and is strictly positive at other points
			\item For $f(x,y) = ax^{2} + bxy + cy^{2}$ to be positive definite
			
			$a,c > 0$ and $ac > b^{2}$
			\item If $ac = b^{2}$ then $f(x,y)$ is positive semi-definite($a>0$) or negative semi definite($a<0$)
			\item If $ac < b^{2}$ then $(0,0)$ is saddle point
			\item $f(x,y) = v^{T}Av$, where $v = \begin{bmatrix}x\\
			y\end{bmatrix}$ and $A = \begin{bmatrix}a&b\\
			b&c\end{bmatrix}$
			
			$|A| < 0\implies$ saddle point, eigenvalues of $A$ are positive if $f(x,y)$ is positive definite
			
			$|A| = 0\implies$ semi definite
		\end{itemize}
	\end{enumerate}
 \section{Week 7}
 \begin{enumerate}
     \item Principal Component Analysis
     \begin{itemize}
         \item Start with as many features as you can collect, and then find a good subset of features. Project the data onto a lower dimensional subspace such that Reconstruction error is minimized, Variation of projected error is maximized.
         \item Actual: $x_i = \sum_{j=1}^d (x_i^Tu_j)u_j$, Projected: $\Tilde{x}=\sum_{j=1}^mz_{ij}u_j + \sum_{j=m+1}^d\beta_ju_j$
         \item Loss function J = $\frac{1}{n}\sum_{i=1}^n||x_i-\Tilde{x}_i||^2$\\
         Differentiating and setting to 0 we get $z_{ij} = x_i^Tu_j$ and $\beta_j = \Bar{x}_j^Tu_j$
         \item So for a given m dimensional subspace spanned by B = \{$u_1,u_2,...,u_m$\} the projected data is\\
         $\Tilde{x}_i = \sum_{j=1}^m(x_i^Tu_j)u_j + \sum_{j=m+1}^d(\Bar{x}^Tu_j)u_j$\\
         Loss $J^* = \sum_{j=m+1}^du_j^TCu_j$, $C = \frac{1}{n}\sum_{i=1}^n(x_i-\Bar{x})(x_i-\Bar{x})^T$, where \{$u_1, u_2,...,u_d$\} are eigenvectors of $C$.\\
         For maximizing variance the maximizer is eigenvector of $C$ corresponding to max eigenvalue, the max variance is also equal to this eigenvalue.
     \end{itemize}
     \item PCA in higher dimension
     \begin{itemize}
         \item Suppose D = \{$x_1,x_2,...,x_n$\} where $x_i\epsilon R$ and $d > > n$, it would be easier to handle a n by n matrix rather than a d by d matrix.
         \item $C = \frac{1}{n}\sum (x_i-\Bar{x})(x_i-\Bar{x})^T = \frac{1}{n}\sum A^TA$ is a d by d matrix.
         \item Since rank($C$)$\leq n\implies(d-n)$ eigenvalues are 0, Hence it is enough to find eigenvectors of $C = \frac{1}{n}AA^T$ which is a n by n matrix.
     \end{itemize}
 \end{enumerate}
 \section{Week 8}
 \begin{enumerate}
     \item Introduction to Optimization
     \begin{itemize}
         \item Pillars of ML: Linear Algebra, Probability and Optimization.
         \item We care about finding the "best" classifier, "least" loss, "maximizing" reward
     \end{itemize}
     \item Solving an Unconstrained Optimization Problem
     \begin{itemize}
         \item We want to minimize $f(x)$ 
         \item We start with $x_0$ (arbitrary choice), then for $t = 0,1,2,...,T$ we update $x_{t+1} = x_t +d$, where $d$ is the direction.
         \item $d=-\alpha f'(x)$, $\alpha$ = STEP SIZE, Gradient Descent converges to local minima
         \item Convex function: Functions in which local minima $\equiv$ global minima
         \item Taylor Series $f(x+\eta d) = f(x) + \eta df'(x) + \frac{\eta^2d^2}{2}f"(x) + ...$
         \item For higher dimensions derivative becomes gradient
         \item Newtons method update rule $x_{n+1} = x_n - \frac{f'(x_n)}{f"(x_n)}$, For higher dimension requires computing Hessian Matrix, If it is not invertible then this method cannot be applied. This method may not converge and would either enter infinite cycle or converge to saddle point instead of minima. It takes more time per iteration, is more computationally intensive and memory intensive.
     \end{itemize}
 \end{enumerate}
 \section{Week 9}
 \begin{enumerate}
     \item Constrained Optimization
     \begin{itemize}
         \item Minimize $f(x)$ such that $g(x)\leq 0$
         \item To check if any $x^*$ is a feasible solution we check $g(x^*)\leq 0$ and NO "descent direction" should be a "feasible direction".
         \item Descent direction: Any direction that reduces our functions value, d is a descent direction if $d^T\nabla f(x^*)<0$.
         \item Feasible direction: Any direction that takes to a point which satisfies all constraints, d is a feasible direction if $d^T\nabla g(x^*)<0$.
         \item Necessary condition for optimal solution: $\nabla f(x^*)=-\lambda \nabla g(x^*)$, $\lambda$ is positive
         \item In equality case $\lambda$ can be negative or positive.
    \end{itemize}
    \item Convexity
    \begin{itemize}
        \item A set $S\subseteq R^d$ is a convex set if $\forall x_1,x_2\epsilon S$ then $\lambda x_1 + (1-\lambda)x_2\epsilon S$
        \item Intersection of convex sets is also a convex set.
        \item $z\epsilon R^d = \sum \lambda_ix_i$ is a convex combination of points in $S$ if $\lambda_i \geq 0$ and $\sum \lambda = 1$.\\
        The set of all such combinations is called Convex Hull(S)
        \item Euclidean Balls in $R^d$: \{$x: ||x||_2\leq \theta$\} where $||x||_2 = \sqrt{\sum x_i^2}$
    \end{itemize}
    \item Convex functions
    \begin{itemize}
        \item $f:r^d\rightarrow R$, $R^d$: any convex set, define epi(f) = $[x z]\epsilon R^{d+1}$ where $z\geq f(x)$.\\
        f is a convex function if epi(f) is a convex set.
        \item f is a convex function iff $\forall x_1,x_2\epsilon R^d$ and all $\lambda \epsilon [0,1]$\\
        $f(\lambda x_1 + (1-\lambda)x_2)\leq \lambda f(x_1) + (1-\lambda)f(x_2)$
        \item Assuming f is differentiable the f is convex iff\\
        $f(y)\geq f(x) + (y - x)^T\nabla f(x)$
        \item If f is twice differentiable, $H\epsilon R^{dxd}$, $H_{ij} = \frac{\delta f}{\delta x_i\delta x_j}$\\
        f is convex iff H is positive semi definite matrix; eigenvalue(H)$\geq 0$
        \item If f is a convex function, then all local minima of f are also global minima
    \end{itemize}
 \end{enumerate}
 \section{Week 10}
 \begin{enumerate}
     \item Properties of convex functions
     \begin{itemize}
         \item If f and g are both convex then f + g is also convex
         \item If f is convex and non decreasing and g is convex then f(g()) is also convex
         \item If f is convex and g is linear then fog is convex
         \item In general if f and g are convex then fog may not be convex.
     \end{itemize}
     \item Analytical Solution for Linear Regression: $w = (X^TX)^-1(X^Ty)$
     \item Constrained Optimization
     \begin{itemize}
         \item minimize $f(x)$ such that $h(x)\leq 0$
         \item Lagrangian function $L(x,\lambda) = f(x) + \lambda h(x)$, where $\lambda$ is a scalar.\\
         For $h(x)\leq 0$, the max$_{\lambda \geq 0}L(x,\lambda) = f(x)$ with $\lambda = 0$\\
         For $h(x) > 0$, the max$_{\lambda \geq 0}L(x,\lambda) = \infty$ with $\lambda = \infty$
         \item min$_xf(x)$ = min$_x$max$_{\lambda \geq 0} L(x,\lambda)$, the DUAL would be max$_{\lambda \geq 0}$min$_x L(x,\lambda)$, where min$_x L(x,\lambda)$ is an unconstrained problem.
         \item $g(\lambda)$ = min$_xf(x) + \lambda h(x)$ is a convex
     \end{itemize}
     \item Relation between PRIMAL and DUAL
     \begin{itemize}
         \item $g(\lambda^*)\leq f(x^*)$, value at DUAL optimum $\leq$ value at PRIMAL optimum (WEAK DUALITY)
         \item if f and h are convex then STRONG DUALITY holds
         \item KKT conditions for constrained optimization\\
         $\nabla f(x^*) + \lambda^*\nabla h(x^*) = 0$ Stationary condition\\
         $\lambda^*h(x^*) = 0$ Complimentary Slackness condition\\
         $h(x^*) \leq 0$ PRIMAL feasibility\\
         $\lambda^* \geq 0$ DUAL feasibility\\
         In general if $(x^*,\lambda^*)$ satisfies above conditions $\implies$ Local Optima
     \end{itemize}
     \item Support Vector Machine
     \begin{itemize}
         \item min $\frac{1}{2}||w||^2$ such that $w^Tx_iy_i\geq 1$
         \item Data set: \{$(x_1,y_1),(x_2,y_2),...,(x_n,y_n)$\}
     \end{itemize}
 \end{enumerate}
 \end{document}
